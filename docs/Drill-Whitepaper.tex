\documentclass{article}
\usepackage{graphicx}

% TODO: why indexing is bad: updatedb, most recent files and so on
% TODO: why depth-first should not be used even for priority files
% TODO: short file names
% TODO: entropy filename

\newcommand{\bigO}{\mathcal{O}}
\begin{document}

\title
{
    Drill 
    \linebreak 
    Heuristic approach to search files produced by humans inside 
    tree-based filesystems
}
\author{Federico Santamorena}
\maketitle

\begin{abstract}
% Nowadays the average user accumulates a vast majority of data, and if a file is needed it's starting to get cheaper to just search for it once in a while instaed of organizing that data.
% Current solutions for PCs and smartphones generally use an indexing solution, first they scan all the files and then keep a database for subsequent searches.
% This method has fallacies and I propose a heuristic crawling solution that beats any other classical search in terms of time and space complexity.
% Using the D language some heuristic solutions are implemented: threaded search, prioritization of files that seem produced by humans, and blacklisting files that seem machine-generated.
% Results show how other software is slower and not scalable.
% Search files without indexing, but clever crawling
\end{abstract}


\section{Acknowledgments}


\section{Introduction}
Everyday we struggle with the vast majority of data we hoard, an interesting fact is that in a lot of cases the cost to organize that data is a lot more expensive than actually searching once in a while what we need inside it.
Searching inside a file system with random files can be summarized in:

\begin{equation}
    \label{equation_introduction}
    \bigO(n)
\end{equation}

And the situation gets even worse if a user has multiple attached disks: 

\begin{equation}
    \label{equation_disks}
    \bigO(n*k)
\end{equation}

With $n$ being the average number of files and $k$ being the number of disks attached.
We need a clever way to crawl that vast amount of data instead of wasting time organizing it and indexing it.

\section{Materials and methods}

\subsection{Multi-threading}
The first important step to weaken [\ref{equation_disks}] is to implement multi-threading.
Right now (2019) an average desktop CPU has between 4-8 threads, it's fairly good to assume that the average user has a number of attached disks that is at max the number of threads his/her CPU can support.
I was perplexed to find that classical tools like find and updatedb still don't use multithreading and have a [\ref{equation_disks}] worst case scenario, by implementing multi-threading and spawning at least 1 thread per physical disk we can have a $\bigO(n)$ time, with $n$ being the number of files an average user has on each disk.
The formula can now be updated to:

\begin{equation}
    \label{equation_multithreading}
    \bigO(n)
\end{equation}



\subsection{Analysis of depth produced by humans}

The first important heuristic approach is to analyze how humans produce data, how they store it and how to improve heuristics.
The average human stores files without nesting too many folders, even myself being a hoarder with a lot of random data in multiple disks I noticed a maximum depth of about 10 folders from the root of the filesystem.
% A short script written in Python analyzing my big 4TB archive with random stuff will show an average depth of 10 folders:
% The average folder has about 100 files.
With $F$ being the average number of folders in a folder.
With $D$ being the average depth humans store data.

The formula can now be updated to:

\begin{equation}
    \label{equation_depth_analysis}
    \bigO(F^D)
\end{equation}



\subsection{Crawling algorithms}

The main two crawling systems programmers are aware of are depth-first search and breadth-first search, they have their advantages and disadvantages, Drill implemented both and in the Results section I will discuss which one is better for searching files produced by humans.


\subsubsection{Depth-first}

Depth-first in summary works by entering the first folder it finds and doing this recursively, until it reaches a leaf node and then going back up on the stack and continuing this drilling procedure.


\subsubsection{Breadth-first}

Breadth-first in summary works by scanning your tree structure by levels of depthness, until it depletes all deep nodes not being leaves.


\subsection{Black-hole folders}

After implementing the multi-threaded solution with depth-first search checking the logs of the crawling will show a very interesting phenomenon: crawlers waste a lot of time in folders generated by the machine, like package managers with never-ending nested folders or just a folder with millions of file like shaders cache.
The first important step to weaken $\bigO(n)$ is to prevent th






\subsection{Entropy of file name}





\subsection{Regex prioritization}

A simple analysis of human produced files like 
The first heuristic solution is to use a dicrionary 

\subsubsection{Dictionary}
\subsubsection{Path-based}








\section{Results}

\subsection{Other software}

\subsubsection{find}

\subsubsection{locate}

\subsection{Drill}

\section{Discussion}

\section{Literature Cited}



\section{Appendices}

\begin{equation}
    \label{simple_equation}
    \alpha = \bigO(n\log n)
\end{equation}



\begin{figure}
    \centering
    % \includegraphics[width=3.0in]{myfigure}
    \caption{Simulation Results}
    \label{simulationfigure}
\end{figure}

\section{Conclusion}
Write your conclusion here.

\end{document}